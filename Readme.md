<h1>ğŸ”§ [1D classification] Fault Diagnosis using CNN</h1>

<p>This project implements a complete modular pipeline for classifying bearing faults using <strong>Convolutional Neural Networks</strong> on the <strong>CWRU</strong> (Case Western Reserve University) dataset.</p>

<hr/>

<h2>ğŸ“ Project Structure</h2>

<pre>
cnn_pipeline/
â”œâ”€â”€ main.py                  # Main training/evaluation script
â”œâ”€â”€ arguments.py             # Command-line arguments
â”œâ”€â”€ dataset.py               # DataLoader generation
â”œâ”€â”€ model.py                 # CNN model definition
â”œâ”€â”€ train.py                 # Training function (with early stopping)
â”œâ”€â”€ test.py                  # Test function
â”œâ”€â”€ preprocess.py            # CWRU .mat file preprocessing
</pre>

<hr/>

<h2>ğŸ“¦ Data Structure</h2>

<pre>
raw_data/                    # Original CWRU .mat files
â”œâ”€â”€ 97.mat                   # Normal
â”œâ”€â”€ 105.mat                  # IR fault
â”œâ”€â”€ 118.mat                  # Ball fault
â”œâ”€â”€ 130.mat                  # Outer race fault
</pre>

<p>The <code>preprocess.py</code> module converts raw signals into sliding window segments for supervised classification.</p>

<hr/>

<h2>ğŸš€ How to Run</h2>

<pre><code>pip install numpy scipy torch scikit-learn matplotlib tqdm</code></pre>

<pre><code>python main.py</code></pre>

<p>The script will:</p>
<ol>
  <li>Preprocess raw <code>.mat</code> files into (X, Y) arrays</li>
  <li>Split data into train/valid/test</li>
  <li>Train CNN using early stopping</li>
  <li>Evaluate final test accuracy</li>
</ol>

<hr/>

<h2>ğŸ§  CNN Architecture</h2>

<p>Three-layer 1D convolutional encoder with ReLU, BatchNorm, and MaxPooling followed by fully connected classification layers.</p>

<pre>
Input â†’ Conv1d â†’ ReLU â†’ MaxPool â†’ Conv1d â†’ ReLU â†’ MaxPool â†’ Conv1d â†’ Flatten â†’ FC â†’ FC â†’ Output
</pre>

<hr/>

<h2>ğŸ“ˆ Training Options (config.py)</h2>

<table>
  <tr><th>Argument</th><th>Description</th><th>Default</th></tr>
  <tr><td><code>--epochs</code></td><td>Number of training epochs</td><td>10</td></tr>
  <tr><td><code>--lr</code></td><td>Learning rate</td><td>1e-4</td></tr>
  <tr><td><code>--lamda</code></td><td>LR scheduler decay</td><td>0.97</td></tr>
  <tr><td><code>--early_stop</code></td><td>Early stopping patience</td><td>20</td></tr>
  <tr><td><code>--train_size</code></td><td>Train/val split ratio</td><td>0.8</td></tr>
  <tr><td><code>--batch_size</code></td><td>Mini-batch size</td><td>64</td></tr>
</table>

<hr/>

